# 分类-逻辑回归

之前介绍的线性回归是针对连续数据的，接下来介绍关于离散数据的机器学习算法。



## 现实中有关分类的例子

邮件： 垃圾/非垃圾邮件？

在线交易：是否欺诈？

肿瘤：恶性还是良性？


$$
y \in \{0 ,1\}     \quad; 0:"负类"，1：“正类”
$$




负类通常表示缺少某种东西，正类表示存在我们想找的东西。这里我们只讨论只有两类（two class or binary classification problem）的问题，以后会介绍有多个分类(multi-classification)的问题。



## 要解决的问题



![figure](./images/6_1-1.png)

如上图所示，当没有最右边那个点时，我们使用粉红线（线性回归）去拟合数据，能够很好的区分良性和恶性肿瘤，如大于0.5的为恶性；二加入了最右边的那个点后，再用一条蓝色线去拟合数据，这时候发现分类就有很大问题，恶性肿瘤被划分到良性分类里了。由此我们发现线性回归针对这类问题不再适用，由此引出了逻辑回归。

## Logistic Regression

逻辑回归的预测总会处于0和1之间：
$$
0\leq h_\theta(x) \leq1
$$




### 假设的函数表达式



为了使得假设函数的值位于0和1之间：
$$
h_\theta(x) = g(\theta^Tx)
$$
其中g为sigmoid函数：
$$
g(z) = \frac{1}{1+e^{-z}}
$$
合到一起就是：
$$
g(z) = \frac{1}{1+e^{-{\theta^Tx}}}
$$


sigmoid函数的图像：

![sigmoid](./images/6_1-2.png)
$$
h_\theta(x) = 对给定输入x评估其结果y=1的概率
$$


